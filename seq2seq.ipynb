{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sourced primarily from https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py.\n",
    "Processing of our specific data sources is our own code.\n",
    "Other sources noted inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.estimator package not installed.\n",
      "tf.estimator package not installed.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.optimizers import RMSprop, Nadam\n",
    "from keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "from numpy.random import choice as random_choice, randint as random_randint, shuffle as random_shuffle, seed as random_seed, rand\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 195966\n",
      "Number of unique input tokens: 36\n",
      "Number of unique output tokens: 38\n",
      "Max sequence length for inputs: 20\n",
      "Max sequence length for outputs: 22\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "def add_input_target_pair(input_text, target_text):\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "            \n",
    "# Sourced from https://github.com/MajorTal/DeepSpell/blob/master/keras_spell.py\n",
    "CHARS = list(string.ascii_lowercase)\n",
    "def add_noise_to_string(a_string, amount_of_noise):\n",
    "    \"\"\"Add some artificial spelling mistakes to the string\"\"\"\n",
    "    if rand() < amount_of_noise * len(a_string):\n",
    "        # Replace a character with a random character\n",
    "        random_char_position = random_randint(len(a_string))\n",
    "        a_string = a_string[:random_char_position] + random_choice(CHARS[:-1]) + a_string[random_char_position + 1:]\n",
    "    if len(a_string) > 1 and rand() < amount_of_noise * len(a_string):\n",
    "        # Delete a character\n",
    "        random_char_position = random_randint(len(a_string))\n",
    "        a_string = a_string[:random_char_position] + a_string[random_char_position + 1:]\n",
    "    if rand() < amount_of_noise * len(a_string):\n",
    "        # Add a random character\n",
    "        random_char_position = random_randint(len(a_string))\n",
    "        a_string = a_string[:random_char_position] + random_choice(CHARS[:-1]) + a_string[random_char_position:]\n",
    "    if len(a_string) > 1 and rand() < amount_of_noise * len(a_string):\n",
    "        # Transpose 2 characters\n",
    "        random_char_position = random_randint(len(a_string) - 1)\n",
    "        a_string = (a_string[:random_char_position] + a_string[random_char_position + 1] + a_string[random_char_position] +\n",
    "                    a_string[random_char_position + 2:])\n",
    "    return a_string\n",
    "\n",
    "# Format data sources as a list of input->output samples - tab separated            \n",
    "lines = []\n",
    "\n",
    "with open('data/common-eng-words.txt') as f:\n",
    "    raw = [line.strip() for line in f.read().lower().split('\\n')]\n",
    "    for line in raw:\n",
    "        # Change this to add more misspellings per word\n",
    "        for i in range(10):\n",
    "            noised = add_noise_to_string(line, 0.05)\n",
    "            if noised != line:\n",
    "                lines.append(noised + '\\t' + line)\n",
    "    \n",
    "with open('data/aspell.txt', 'r') as f:\n",
    "    raw = [line.strip() for line in f.read().lower().replace('_', ' ').split('\\n')]\n",
    "    processed = []\n",
    "    raw_target = ''\n",
    "    for line in raw:\n",
    "        if line.startswith('$'):\n",
    "            raw_target = line.replace('$', '')\n",
    "        elif line != raw_target:\n",
    "            processed.append(line + '\\t' + raw_target)\n",
    "    lines += processed\n",
    "\n",
    "with open('data/wikipedia-misspellings.txt', 'r') as f:\n",
    "    lines += [line.strip().replace('->','\\t') for line in f.read().lower().split('\\n')]\n",
    "    \n",
    "# Make unique\n",
    "lines = list(set(lines))\n",
    "    \n",
    "# Split raw lines into inputs and outputs\n",
    "for line in lines:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    # Also needed to split target on commas, since some misspellings\n",
    "    # have multiple targets in the data\n",
    "    for target in target_text.split(','):\n",
    "        target_with_tokens = '\\t' + target.strip() + '\\n'\n",
    "        add_input_target_pair(input_text, target_with_tokens)\n",
    "        # Add identity\n",
    "        if target not in input_texts:\n",
    "            add_input_target_pair(target, target_with_tokens)\n",
    "\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "# encoder_input_data is a 3D array of shape (num_pairs, max_english_sentence_length, num_english_characters) \n",
    "# containing a one-hot vectorization of the English sentences.\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition, training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 156772 samples, validate on 39194 samples\n",
      "Epoch 1/50\n",
      "156772/156772 [==============================] - 163s 1ms/step - loss: 0.4663 - val_loss: 0.2466\n",
      "Epoch 2/50\n",
      "156772/156772 [==============================] - 158s 1ms/step - loss: 0.2741 - val_loss: 0.2033\n",
      "Epoch 3/50\n",
      "156772/156772 [==============================] - 156s 995us/step - loss: 0.2422 - val_loss: 0.1864\n",
      "Epoch 4/50\n",
      "156772/156772 [==============================] - 151s 961us/step - loss: 0.2251 - val_loss: 0.1748\n",
      "Epoch 5/50\n",
      "156772/156772 [==============================] - 150s 955us/step - loss: 0.2136 - val_loss: 0.1694\n",
      "Epoch 6/50\n",
      "156772/156772 [==============================] - 153s 976us/step - loss: 0.2044 - val_loss: 0.1645\n",
      "Epoch 7/50\n",
      "156772/156772 [==============================] - 150s 960us/step - loss: 0.1982 - val_loss: 0.1598\n",
      "Epoch 8/50\n",
      "156772/156772 [==============================] - 151s 964us/step - loss: 0.1924 - val_loss: 0.1555\n",
      "Epoch 9/50\n",
      "156772/156772 [==============================] - 152s 969us/step - loss: 0.1879 - val_loss: 0.1531\n",
      "Epoch 10/50\n",
      "156772/156772 [==============================] - 151s 965us/step - loss: 0.1843 - val_loss: 0.1511\n",
      "Epoch 11/50\n",
      "156772/156772 [==============================] - 151s 964us/step - loss: 0.1801 - val_loss: 0.1492\n",
      "Epoch 12/50\n",
      "156772/156772 [==============================] - 151s 963us/step - loss: 0.1770 - val_loss: 0.1461\n",
      "Epoch 13/50\n",
      "156772/156772 [==============================] - 154s 980us/step - loss: 0.1741 - val_loss: 0.1457\n",
      "Epoch 14/50\n",
      "156772/156772 [==============================] - 152s 972us/step - loss: 0.1715 - val_loss: 0.1443\n",
      "Epoch 15/50\n",
      "156772/156772 [==============================] - 152s 967us/step - loss: 0.1694 - val_loss: 0.1431\n",
      "Epoch 16/50\n",
      "156772/156772 [==============================] - 152s 970us/step - loss: 0.1667 - val_loss: 0.1430\n",
      "Epoch 17/50\n",
      "156772/156772 [==============================] - 151s 965us/step - loss: 0.1649 - val_loss: 0.1405\n",
      "Epoch 18/50\n",
      "156772/156772 [==============================] - 152s 969us/step - loss: 0.1628 - val_loss: 0.1402\n",
      "Epoch 19/50\n",
      "156772/156772 [==============================] - 152s 967us/step - loss: 0.1615 - val_loss: 0.1400\n",
      "Epoch 20/50\n",
      "156772/156772 [==============================] - 152s 969us/step - loss: 0.1591 - val_loss: 0.1395\n",
      "Epoch 21/50\n",
      "156772/156772 [==============================] - 151s 963us/step - loss: 0.1586 - val_loss: 0.1387\n",
      "Epoch 22/50\n",
      "156772/156772 [==============================] - 151s 963us/step - loss: 0.1566 - val_loss: 0.1385\n",
      "Epoch 23/50\n",
      "156772/156772 [==============================] - 153s 976us/step - loss: 0.1550 - val_loss: 0.1385\n",
      "Epoch 24/50\n",
      "156772/156772 [==============================] - 155s 990us/step - loss: 0.1540 - val_loss: 0.1380\n",
      "Epoch 25/50\n",
      "156772/156772 [==============================] - 155s 986us/step - loss: 0.1533 - val_loss: 0.1354\n",
      "Epoch 26/50\n",
      "156772/156772 [==============================] - 153s 975us/step - loss: 0.1520 - val_loss: 0.1350\n",
      "Epoch 27/50\n",
      "156772/156772 [==============================] - 152s 972us/step - loss: 0.1502 - val_loss: 0.1339\n",
      "Epoch 28/50\n",
      "156772/156772 [==============================] - 154s 982us/step - loss: 0.1495 - val_loss: 0.1352\n",
      "Epoch 29/50\n",
      "156772/156772 [==============================] - 154s 982us/step - loss: 0.1489 - val_loss: 0.1347\n",
      "Epoch 30/50\n",
      "156772/156772 [==============================] - 153s 976us/step - loss: 0.1474 - val_loss: 0.1342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tools\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_32 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_31/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_31/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "params = dict(\n",
    "    dropout = 0.1,\n",
    "    recurrent_dropout = 0.0,\n",
    "    unroll = False,\n",
    "    implementation = 2\n",
    ")\n",
    "\n",
    "epochs = 50  # Number of epochs to train for.\n",
    "batch_size = 64  # Batch size for training.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True, \n",
    "               dropout=params['dropout'], recurrent_dropout=params['recurrent_dropout'],\n",
    "               unroll=params['unroll'], implementation=params['implementation'])\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,\n",
    "                    dropout=params['dropout'], recurrent_dropout=params['recurrent_dropout'],\n",
    "                    unroll=params['unroll'], implementation=params['implementation'])\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer=Nadam(lr=0.003), loss='categorical_crossentropy')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping]\n",
    "                   )\n",
    "# Save model\n",
    "import datetime\n",
    "unique_string = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model.save('models/' + unique_string + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bug in matplotlib, needed to update the package and restart the kernel, so I just hard-coded the history before doing so\n",
    "loss = [0.4663085410406544, 0.2740548289212563, 0.2422146208798551, 0.22514867456077395, 0.21356780256708086, 0.20440667601824994, 0.19821522371479938, 0.19238745212080444, 0.1879354350587638, 0.18430179420831913, 0.18005238312705021, 0.17695717890529108, 0.17406940066736556, 0.17148588207347581, 0.16938364475161757, 0.1666604769039637, 0.1649325997457815, 0.16277361474422722, 0.1615113084501122, 0.15909200022941652, 0.1585824241980941, 0.15661183741114096, 0.15498967959880341, 0.1540248249532818, 0.1533214237932661, 0.1520381962441219, 0.1501685483513509, 0.14952897805111012, 0.14893947404803853, 0.14744972091809658]\n",
    "val_loss = [0.24663482333054473, 0.20333886761996747, 0.18636745432082955, 0.17484174000871372, 0.16938115689376365, 0.16452781894155052, 0.1597688032957938, 0.15550810431390233, 0.1531175080497283, 0.15112573627264467, 0.14920654876497247, 0.1460789481259952, 0.1457458000943607, 0.14430301011159188, 0.14309628132095614, 0.14296739501719416, 0.1405173539719959, 0.14020586447058306, 0.13998657333587466, 0.13953082218455218, 0.13866029423831427, 0.13850701391931128, 0.1385034767385891, 0.1379971048181298, 0.13535009278471619, 0.13503757405689848, 0.1339019722748017, 0.13521093099012163, 0.13468205619099835, 0.13418629105348895]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(loss)\n",
    "plt.plot(val_loss)\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Misspellings as input--\n",
      "acheive -> achieve\n",
      "\n",
      "apparantly -> apparently\n",
      "\n",
      "enviroment -> environment\n",
      "\n",
      "goverment -> government\n",
      "\n",
      "persistant -> persistent\n",
      "\n",
      "religous -> religious\n",
      "\n",
      "wierd -> wired\n",
      "\n",
      "truely -> truly\n",
      "\n",
      "suprise -> surprise\n",
      "\n",
      "occurence -> occurrence\n",
      "\n",
      "--------------------------\n",
      "--Correct words as input--\n",
      "immediately -> immediately\n",
      "\n",
      "gist -> gist\n",
      "\n",
      "embarrass -> embarrassed\n",
      "\n",
      "committee -> committee\n",
      "\n",
      "assassination -> assassination\n",
      "\n",
      "across -> across\n",
      "\n",
      "business -> business\n",
      "\n",
      "guard -> guard\n",
      "\n",
      "knowledge -> knowledge\n",
      "\n",
      "millennia -> millennia\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_inference(test_words):\n",
    "    encoder_input_test_data = np.zeros(\n",
    "        (len(test_words), max_encoder_seq_length, num_encoder_tokens),\n",
    "        dtype='float32')\n",
    "\n",
    "    for i, word in enumerate(test_words):\n",
    "        for t, char in enumerate(word):\n",
    "            if char in input_token_index:\n",
    "                encoder_input_test_data[i, t, input_token_index[char]] = 1.\n",
    "\n",
    "    for idx in range(len(encoder_input_test_data)):\n",
    "        print(test_words[idx],'->', decode_sequence(encoder_input_test_data[idx:idx+1]))\n",
    "        \n",
    "print(\"--Misspellings as input--\")\n",
    "test_inference([\n",
    "    'acheive', 'apparantly', 'enviroment', 'goverment', 'persistant',\n",
    "    'religous', 'wierd', 'truely', 'suprise', 'occurence'\n",
    "])\n",
    "print(\"--------------------------\")\n",
    "print(\"--Correct words as input--\")\n",
    "test_inference([\n",
    "    'immediately', 'gist', 'embarrass', 'committee', 'assassination',\n",
    "    'across', 'business', 'guard', 'knowledge', 'millennia'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
